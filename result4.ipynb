{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175f8aab",
   "metadata": {},
   "source": [
    "## 基于resnet50的图像识别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# 加载预训练的CNN模型\n",
    "def load_cnn_model():\n",
    "    # ResNet50特征提取\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    model_path = './model/resnet50-0676ba61.pth'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    # 移除最后的全连接层，只保留特征提取部分\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()  \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)  \n",
    "    return model, device\n",
    "\n",
    "# 定义图像预处理流程\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "# 计算单个图片的CNN特征\n",
    "def compute_image_features(image_path, model, device, transform):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():  # 提取特征\n",
    "            features = model(image_tensor)\n",
    "            \n",
    "        # 将特征张量转换为一维数组\n",
    "        features = features.squeeze().cpu().numpy()\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"计算图片特征时出错 {image_path}: {e}\")\n",
    "        return np.zeros(2048)  # 特征维度为2048\n",
    "\n",
    "# 计算两个特征向量之间的相似度\n",
    "def compute_feature_similarity(features1, features2):\n",
    "    try:\n",
    "        # 余弦相似度（1-余弦距离）\n",
    "        similarity = 1 - cosine(features1, features2)\n",
    "        return similarity\n",
    "    except Exception as e:\n",
    "        print(f\"计算特征相似度时出错: {e}\")\n",
    "        return 0\n",
    "\n",
    "# 查找相似图片\n",
    "def find_similar_images_sampled(all_papers_content, paper_names, team_ids, team_image_counts, base_dir=\"./data/transform/\", sample_size=2000): \n",
    "    # 加载CNN模型\n",
    "    print(\"加载预训练的CNN模型...\")\n",
    "    model, device = load_cnn_model()\n",
    "    transform = get_transform()\n",
    "    \n",
    "    # 收集所有图片信息\n",
    "    image_info = []\n",
    "    \n",
    "    total_papers = len(all_papers_content)\n",
    "    \n",
    "    for paper_idx, paper_content in enumerate(tqdm(all_papers_content, desc=\"处理论文\")):\n",
    "        paper_desc = f\"处理第 {paper_idx+1}/{total_papers} 篇论文\"\n",
    "        \n",
    "        for item_idx, item in enumerate(paper_content):\n",
    "            if item['type'] == 'image':\n",
    "                if os.path.isabs(item['img_path']):\n",
    "                    img_path = item['img_path']\n",
    "                else:\n",
    "                    img_path = os.path.normpath(item['img_path'])\n",
    "                \n",
    "                if not os.path.exists(img_path):\n",
    "                    print(f\"\\r{paper_desc} - 图片不存在: {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # 计算图片的页内序号\n",
    "                in_page_idx = sum(1 for x in paper_content[:item_idx] \n",
    "                               if x['type'] == 'image' and x['page_idx'] == item['page_idx'])\n",
    "                \n",
    "                # 计算图片CNN特征\n",
    "                image_features = compute_image_features(img_path, model, device, transform)\n",
    "                \n",
    "                info = {\n",
    "                    'paper_idx': paper_idx,\n",
    "                    'team_id': team_ids[paper_idx],\n",
    "                    'item_idx': item_idx,\n",
    "                    'page_idx': item['page_idx'],\n",
    "                    'img_path': img_path,\n",
    "                    'in_page_idx': in_page_idx,\n",
    "                    'features': image_features\n",
    "                }\n",
    "                image_info.append(info)\n",
    "    \n",
    "    total_images = len(image_info)\n",
    "    print(f\"共收集和处理了 {total_images} 张图片\")\n",
    "    \n",
    "    if total_images == 0:\n",
    "        print(\"没有找到图片，跳过比较\")\n",
    "        return []\n",
    "    \n",
    "    actual_sample_size = min(sample_size, total_images)\n",
    "    # 随机抽样\n",
    "    sampled_images = random.sample(image_info, actual_sample_size)\n",
    "    \n",
    "    similar_images = []\n",
    "    similarity_threshold = 0.95  # CNN特征使用更高的阈值\n",
    "    \n",
    "    total_comparisons = (actual_sample_size * (actual_sample_size - 1)) // 2\n",
    "\n",
    "    progress_bar = tqdm(total=total_comparisons, desc=\"比较图片相似度\")\n",
    "    \n",
    "    for i in range(len(sampled_images)):\n",
    "        img1 = sampled_images[i]\n",
    "        paper_idx1 = img1['paper_idx']\n",
    "        features1 = img1['features']\n",
    "        \n",
    "        for j in range(i+1, len(sampled_images)):\n",
    "            img2 = sampled_images[j]\n",
    "            paper_idx2 = img2['paper_idx']\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "        \n",
    "            if paper_idx1 == paper_idx2:\n",
    "                continue\n",
    "            \n",
    "            # 计算相似度\n",
    "            features2 = img2['features']\n",
    "            similarity = compute_feature_similarity(features1, features2)\n",
    "            \n",
    "            if similarity > similarity_threshold:\n",
    "                print(f\"\\n发现相似图片!\")\n",
    "                similar_images.append({\n",
    "                    '参赛队号': img1['team_id'],\n",
    "                    '雷同图片所在页码': img1['page_idx'],\n",
    "                    '雷同图片的页内序号': img1['in_page_idx'],\n",
    "                    '论文中的图片数量': team_image_counts[img1['team_id']]\n",
    "                })\n",
    "    progress_bar.close()  \n",
    "    return similar_images\n",
    "\n",
    "# 对结果进行去重操作\n",
    "def deduplicate_similar(df):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    deduplicated_df = df.drop_duplicates(\n",
    "        subset=['参赛队号', '雷同图片所在页码', '雷同图片的页内序号'],\n",
    "        keep='first'\n",
    "    )\n",
    "    \n",
    "    return deduplicated_df\n",
    "\n",
    "# 统计每篇论文中的图片数量\n",
    "def count_images_per_paper(all_papers_content, team_ids):\n",
    "    team_image_counts = {}\n",
    "    \n",
    "    for paper_idx, paper_content in enumerate(all_papers_content):\n",
    "        team_id = team_ids[paper_idx]\n",
    "        # 计算该论文中图片的总数\n",
    "        image_count = sum(1 for item in paper_content if item['type'] == 'image')\n",
    "        team_image_counts[team_id] = image_count\n",
    "        \n",
    "    return team_image_counts\n",
    "\n",
    "def main():\n",
    "    os.makedirs(os.path.dirname('./data/result4.xlsx'), exist_ok=True)\n",
    "\n",
    "    print(\"读取附件1的参赛队号和加密号映射...\")\n",
    "    attachment1 = pd.read_excel('./data/附件1.xlsx')\n",
    "    team_id_mapping = dict(zip(attachment1['加密号'], attachment1['参赛队号']))\n",
    "    print(f\"成功加载 {len(team_id_mapping)} 个队伍信息\")\n",
    "    \n",
    "    data_dir = \"./data/transform/\"\n",
    "    print(f\"开始在 {data_dir} 中查找论文文件...\")\n",
    "\n",
    "    paper_files = []\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\"_content_list.json\"):\n",
    "                paper_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"找到 {len(paper_files)} 个论文文件\")\n",
    "    \n",
    "    # 加载所有论文的内容\n",
    "    all_papers_content = []\n",
    "    paper_names = []\n",
    "    team_ids = []  # 存储队伍ID\n",
    "    \n",
    "    for i, paper_file in enumerate(paper_files):  \n",
    "        print(f\"加载第 {i+1}/{len(paper_files)} 篇论文: {paper_file}\")\n",
    "        \n",
    "        # 提取加密码\n",
    "        file_name = os.path.basename(paper_file)\n",
    "        encryption_code = file_name.split('_')[0]\n",
    "        \n",
    "        # 找到对应的队伍ID\n",
    "        team_id = team_id_mapping.get(encryption_code, '未知')\n",
    "        team_ids.append(team_id)\n",
    "        \n",
    "        with open(paper_file, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                paper_content = json.load(f)\n",
    "                all_papers_content.append(paper_content)\n",
    "                paper_names.append(os.path.relpath(paper_file, data_dir))\n",
    "                print(f\"成功加载论文，参赛队号: {team_id}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"解析JSON文件失败: {paper_file}\")\n",
    "                print(f\"错误信息: {str(e)}\")\n",
    "    \n",
    "    # 计算每篇论文中的图片数量\n",
    "    team_image_counts = count_images_per_paper(all_papers_content, team_ids)\n",
    "    print(\"已完成各参赛队论文图片总数统计\")\n",
    "    \n",
    "    similar_images = find_similar_images_sampled(all_papers_content, paper_names, team_ids, team_image_counts, data_dir, sample_size=2000)\n",
    "    similar_images = pd.DataFrame(similar_images)\n",
    "    df_images = deduplicate_similar(similar_images)\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter('./data/result4.xlsx') as writer:\n",
    "            print(\"写入雷同图片工作表...\")\n",
    "            if not df_images.empty:\n",
    "                df_images.to_excel(writer, sheet_name='雷同图片', index=False)\n",
    "            else:\n",
    "                empty_images = pd.DataFrame(columns=['参赛队号', '雷同图片所在页码', '雷同图片的页内序号', '论文中的图片数量'])\n",
    "                empty_images.to_excel(writer, sheet_name='雷同图片', index=False)\n",
    "        print(\"结果已成功写入!\")\n",
    "    except Exception as e:\n",
    "        print(f\"写入Excel文件时出错: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe866f1",
   "metadata": {},
   "source": [
    "## 计算最终论文重复率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b439982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_and_calculate_final_rate(excel_path):\n",
    "    try:\n",
    "        duplication_sheet = pd.read_excel(excel_path, sheet_name='文本重复率')\n",
    "        image_sheet = pd.read_excel(excel_path, sheet_name='雷同图片')\n",
    "        formula_sheet = pd.read_excel(excel_path, sheet_name='雷同公式')\n",
    "\n",
    "        # 获取图片和公式的统计数据\n",
    "        image_data = image_sheet.groupby('参赛队号').agg({\n",
    "            '论文中的图片数量': 'first',  # 保留论文中的图片数\n",
    "            '参赛队号': 'size'  # 计算雷同图片数量\n",
    "        }).rename(columns={'参赛队号': '雷同图片数量'}).reset_index()\n",
    "        \n",
    "        formula_counts = formula_sheet.groupby('参赛队号').size().reset_index(name='雷同公式数量')\n",
    "\n",
    "        # 合并所有数据\n",
    "        combined_df = duplication_sheet.merge(image_data, on='参赛队号', how='left').merge(\n",
    "            formula_counts, on='参赛队号', how='left')\n",
    "        \n",
    "        # 填充缺失值\n",
    "        combined_df['雷同图片数量'] = combined_df['雷同图片数量'].fillna(0).astype(int)\n",
    "        combined_df['雷同公式数量'] = combined_df['雷同公式数量'].fillna(0).astype(int)\n",
    "        combined_df['论文中的图片数量'] = combined_df['论文中的图片数量'].fillna(0).astype(int)\n",
    "\n",
    "        # 根据图片数量调整权重\n",
    "        condition = combined_df['论文中的图片数量'] > 25\n",
    "        \n",
    "        # 应用不同的权重计算\n",
    "        combined_df['重复率'] = 0.0\n",
    "        # 图片数量 > 25 的情况\n",
    "        combined_df.loc[condition, '重复率'] = (\n",
    "            0.6 * combined_df.loc[condition, '文本重复率'] +\n",
    "            0.3 * combined_df.loc[condition, '雷同图片数量'] +\n",
    "            0.1 * combined_df.loc[condition, '雷同公式数量']\n",
    "        )\n",
    "        # 图片数量 <= 25 的情况\n",
    "        combined_df.loc[~condition, '重复率'] = (\n",
    "            0.8 * combined_df.loc[~condition, '文本重复率'] +\n",
    "            0.1 * combined_df.loc[~condition, '雷同图片数量'] +\n",
    "            0.1 * combined_df.loc[~condition, '雷同公式数量']\n",
    "        )\n",
    "\n",
    "        # 保存结果\n",
    "        with pd.ExcelWriter(excel_path, mode='a', engine='openpyxl') as writer:\n",
    "            # writer.book.remove(writer.book['文本重复率'])\n",
    "            result_df = combined_df[['参赛队号', '重复率']]\n",
    "            result_df.to_excel(writer, sheet_name='重复率', index=False)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：文件 '{excel_path}' 未找到\")\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件时出错: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    excel_path = './data/result4.xlsx'\n",
    "    if not os.path.exists(excel_path):\n",
    "        print(f\"错误：文件 '{excel_path}' 不存在\")\n",
    "        return\n",
    "    # 执行合并和计算\n",
    "    combine_and_calculate_final_rate(excel_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a69112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
